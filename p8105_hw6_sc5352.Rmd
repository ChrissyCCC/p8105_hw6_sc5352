---
title: "HW6"
author: "Chrissy Chen"
date: "2023-11-28"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(boot)
library(broom)
library(readr)
library(modelr)
library(mgcv)
library(dplyr)
```


#### Problem 2
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

```{r, warning=FALSE}
set.seed(1)
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

bootstrap_results_betas = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    results = map(models, broom::tidy)) |> 
  select(-strap_sample, -models) |> 
  unnest(results) |>
  select(strap_number, term, estimate) |>
  pivot_wider(names_from = term, values_from = estimate) |>
  mutate(coef = log(tmin * prcp))

proportion_coef_na = 
  bootstrap_results_betas |>
  filter(is.na(coef)) |>
  summarise(count = n()) |>
  mutate(proportion = count/5000)
proportion_coef_na

quantile_betas = 
  bootstrap_results_betas |>
  na.omit() |>
  summarize(
    ci_lower = quantile(coef, 0.025), 
    ci_upper = quantile(coef, 0.975))
quantile_betas



bootstrap_results_rsq = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    results = map(models, broom::glance)) |>
  select(-strap_sample, -models) |> 
  unnest(results) |>
  select(strap_number, r.squared) 

quantile_rsq = 
  bootstrap_results_rsq |>
  summarize(
    ci_lower = quantile(r.squared, 0.025), 
    ci_upper = quantile(r.squared, 0.975))
quantile_rsq

par(mfrow = c(1, 2))
hist(bootstrap_results_rsq$r.squared, main = "Distribution of rsquared", col = "lightgreen")
hist(bootstrap_results_betas$coef, main = "Distribution of log(beta1 * beta2)", col = "lightblue")
par(mfrow = c(1, 1))
```
After taking log of the beta 1 and 2, there are in total 3361 NAs, which account for about 67% of the whole dataset. For those `coef` are significant, the 95% quantile is `r quantile_betas`. 

The 95% quantile for `r_squared` is `r quantile_rsq`.  

#### Problem 3
```{r}
birthweight = 
  read_csv("birthweight.csv")

model_1 = lm(bwt ~ delwt + wtgain + blength, data = birthweight)
prediction_1 = add_predictions(birthweight, model_1)
residual_1 = add_residuals(prediction_1, model_1)

model_lg = lm(bwt ~ blength + gaweeks, data = birthweight)
prediction_lg = add_predictions(birthweight, model_lg)
residual_lg = add_residuals(prediction_lg, model_lg)

model_hls = lm(bwt ~ bhead * blength * babysex, data = birthweight)
prediction_hls = add_predictions(birthweight, model_hls)
residual_hls = add_residuals(prediction_hls, model_hls)

par(mfrow = c(1, 3))
ggplot(residual_1,aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Model_1: Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")
ggplot(residual_lg,aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Model_lg: Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")
ggplot(residual_hls,aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Model_hls: Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")
par(mfrow = c(1, 1))

```
```{r}
cv_df = 
  crossv_mc(birthweight, 100) |>
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) |>
  mutate(
    model_1  = map(train, \(df) lm(bwt ~ delwt + wtgain + blength, data = birthweight)),
    model_lg  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = birthweight)),
    model_hls  = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = birthweight))) |> 
  mutate(
    rmse_1 = map2_dbl(model_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_lg = map2_dbl(model_lg, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_hls = map2_dbl(model_hls, test, \(mod, df) rmse(model = mod, data = df))) |>
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

cv_df
```
My model is `model_1`, which sets `baby’s length at birth (centimeteres)`, `mother’s weight at delivery (pounds)`, and `mother’s weight gain during pregnancy (pounds)` as the predictors.  

The mean of residuals for all three models are around 0.  

Finally, I plotted the prediction error distribution for each candidate model. and the `model_hls` that took `bhead`, `blength`, `babysex`, and their interactions as predictors, has the lowest mse. In the end, I’d probably go with the interaction model.


